Are you capturing the response time of each and every scenario for before and after build.
--

Are you using newly added packages code inside model score function for python and R. (P)
-- Whatever new packages we are installing we need import them in score function
-- If different version of same package are used in cran and conda then which is actually installed
-- Install packages from apt-get
-- Run curl command in init script


Are you testing all the function of mosaic-ai-client/ r client/ sas client ? (P)
-- Test functions like add version, lload model, delete model, ddeploy model - Refer to user guide
"scoring_func",
"add_version",
"delete_model",
"deploy_model",
"apply_model_strategy",
"promote_model",
"describe_model",
"list_models",
"load_model",
"register_model",
"stop_model",
"get_model_profiling",
"generate_schema",
"update_metadata_info",
"update_existing_model",
"update_model_details",
"update_version_details",
"register_ensemble_model",
"ensemble_model_list",
"build_time_metrics",
"load_train_and_test_data",
"fetch_model_resources",
"describe_model_using_model_name",
"get_model_info",
"fetch_feedback_accuracy",
"delete_model_version",
"RegisterModel"


Are you checking for existing model deployment of R , Python and sas ? (P)
-- Bulk inferncing (Minimum 20 25 requests manually)
-- Testing with incorrect payload

Could you please let me know how you are checking the deployment strategies ?
-- Bulk inferncing (Minimum 20 25 requests manually) and payload distribution as per strategy

Are we doing the data comparison of buildtimemetrics, kyd and explainable ?
-- Understanding of metrics
-- Check if the generated values are correct or not

Are we registering the language model with all the template ? (P)
-- Test models with Python 3.6, Python 3.7 separately
-- Test all elements on model UI

We also need to do the comparison of response for model deployment.
-- Comparison of responses in back to back requests
-- Comparison of responses when model is inferenced across multiple days
-- Understanding of features like kyd

On HPA how are we checking

Is export model feature working as expected if we move the model from QA to GA ?
-- We will test from GA 3.1 release for new and old models

We should also capture the memory footprint if you are registering the model from NOTEBOOK
-- Exercise is required to try and register models with all templates for all resource configs

Action items - 
1. We need to proper understading of model features from code perspective
2. Find out gaps in test data
3. Reading user guide
4. Sanity and regression suite update on timely basis
5. Detailed testing scheduler and monitor, workflow features
6. Automl testing with complex dataset
7. scorev2 testing